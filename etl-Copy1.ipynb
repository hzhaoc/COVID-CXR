{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-7cef738b1c52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# once intialized, it's global in the jupyter notebook kernal; one context per notebook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'local[*]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\covid\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\covid\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    121\u001b[0m         \"\"\"\n\u001b[0;32m    122\u001b[0m         if (conf is not None and\n\u001b[1;32m--> 123\u001b[1;33m                 conf.get(\"spark.executor.allowSparkContext\", \"true\").lower() != \"true\"):\n\u001b[0m\u001b[0;32m    124\u001b[0m             \u001b[1;31m# In order to prevent SparkContext from being created in executors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_on_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from src.etl import *\n",
    "from src.utils import *\n",
    "import yaml\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "# once intialized, it's global in the jupyter notebook kernal; one context per notebook\n",
    "sc = pyspark.SparkContext.getOrCreate('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_list = range(10000)\n",
    "rdd = sc.parallelize(big_list, 2)\n",
    "odds = rdd.filter(lambda x: x % 2 != 0)\n",
    "odds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-fee4b19893bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrdd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'rdd' is not defined"
     ]
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-17-445376adb8eb>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-445376adb8eb>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    \"\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SELECT subject_id, sex, dob, ifnull(dod, \"\")\n",
    "FROM PATIENT\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.etl import *\n",
    "from src.utils import *\n",
    "import yaml\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['data']['image_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "META = etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('label distribution: ', Counter(META.label), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "META = dataset_split(META)\n",
    "META.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('label integer map: ', params['train']['labelmap'], sep='\\n', end='\\n\\n')\n",
    "for ds in (0, 1):\n",
    "    print('{0} data from covid datasets: '.format({0: 'test', 1: 'train'}[ds]), Counter(META[META.train==ds].label), sep='\\n', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache image meta\n",
    "if not os.path.isdir(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "with open(os.path.join(SAVE_PATH, 'meta'), 'wb') as pickle_file:\n",
    "    pickle.dump(META, pickle_file, pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"META data saved at {os.path.join(SAVE_PATH, 'meta')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if False:\n",
    "    # -------------------------------------------------------------------------------------------------------------------\n",
    "    # imagge file to matrix  \n",
    "    # NOTE: this needs model to load all data at once into cache which take up too much storage, \n",
    "    #       making computing inefficient, NOT GOOD\n",
    "    # -------------------------------------------------------------------------------------------------------------------\n",
    "    # train_data / test_data structure: \n",
    "    #                        {'covid': {'data': list of size*size*3 numpy array, 'label': corresponding list of integer},\n",
    "    #                        '!covid': {'data': list of size*size*3 numpy array, 'label': corresponding list of integer}}\n",
    "    TRAIN_DATA, TEST_DATA = datafromfile(META)\n",
    "    # a sanity check\n",
    "    print(f\"train covid sample #: {len(TRAIN_DATA['covid']['label'])}\")\n",
    "    print(f\"train !covid sample #: {len(TRAIN_DATA['!covid']['label'])}\")\n",
    "    print(f\"test  covid sample #: {len(TEST_DATA['covid']['label'])}\")\n",
    "    print(f\"test  !covid sample #: {len(TEST_DATA['!covid']['label'])}\")\n",
    "    # cache image data\n",
    "    with open(os.path.join(SAVE_PATH, 'train.data'), 'wb') as pickle_file:\n",
    "        pickle.dump(TRAIN_DATA, pickle_file, pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"Image data saved at {os.path.join(SAVE_PATH, 'train.data')}\")\n",
    "    with open(os.path.join(SAVE_PATH, 'test.data'), 'wb') as pickle_file:\n",
    "        pickle.dump(TEST_DATA, pickle_file, pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"Image data saved at {os.path.join(SAVE_PATH, 'test.data')}\")\n",
    "else:\n",
    "    # merge imgaes together instead, and transform image to batches of matrix during training\n",
    "    mergesoure(META)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for image shape, they need to consistent with model input\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "for i in np.random.choice(META.index, 10):\n",
    "    print(i, cv2.imread(META.loc[i, 'imgid']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
