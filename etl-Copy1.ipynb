{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"COVID-CXR META ETL\") \\\n",
    "    .config(\"spark.some.config.option\", \"xxx\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    # once intialized, it's global in the jupyter notebook kernal; one context per notebook\n",
    "    sc = pyspark.SparkContext.getOrCreate('local[*]')\n",
    "except:\n",
    "    sc = pyspark.SparkContext('local[*]')\n",
    "sqlc = pyspark.SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_etl_META_0():\n",
    "    # src 0\n",
    "    rdd = spark.read.csv(INPUT_PATH_0_META, header=True, sep=\",\").rdd\n",
    "    \n",
    "    global _src0_url\n",
    "    _src0_url = rdd.map(lambda x: x.url).collect()  # RDD ACTION, for duplicates in src 3\n",
    "    \n",
    "    rdd = rdd.map(lambda x: CXR(pid=x.patientid, fn_src=x.filename, label=x.finding, src=0))\n",
    "    rdd = rdd.filter(lambda x: 1 - x.isna())\n",
    "    rdd = rdd.filter(lambda x: x.view in ([\"PA\", \"AP\", \"AP Supine\", \"AP semi erect\", \"AP erect\"]))\n",
    "    rdd = rdd.filter(lambda x: x.label!= 'other')\n",
    "    return rdd\n",
    "\n",
    "def spark_etl_META_1():\n",
    "    # src 1\n",
    "    rdd = spark.read.csv(INPUT_PATH_1_META, header=True, sep=\",\").rdd\n",
    "    rdd = rdd.map(lambda x: CXR(pid=x.patientid, fn_src=None, label=x.finding, src=1))\n",
    "    rdd = rdd.filter(lambda x: 1- x.isna())\n",
    "    rdd = rdd.filter(lambda x: x.label!= 'other')\n",
    "    return rdd\n",
    "\n",
    "def spark_etl_META_2():\n",
    "    # src 2\n",
    "    rdd = spark.read.csv(INPUT_PATH_2_META, header=True, sep=\",\").rdd\n",
    "    rdd = rdd.map(lambda x: CXR(pid=x.patientid, fn_src=x.imagename, label=x.finding, src=2))\n",
    "    rdd = rdd.filter(lambda x: 1- x.isna())\n",
    "    rdd = rdd.filter(lambda x: x.label!= 'other')\n",
    "    return rdd\n",
    "\n",
    "def spark_etl_META_3():\n",
    "    # src 3\n",
    "    df0 = src3_etl(INPUT_PATH_3_0_META, 'covid')\n",
    "    df1 = src3_etl(INPUT_PATH_3_1_META, 'normal')\n",
    "    df2 = src3_etl(INPUT_PATH_3_2_META, 'pneumonia')\n",
    "    META_3 = df0.append(df1).append(df2)\n",
    "    return META_3\n",
    "def spark_src3_etl(metapath, label):\n",
    "    df = pd.read_csv(metapath)\n",
    "    del df['SIZE']\n",
    "    if label=='covid':\n",
    "        # discard: # https://github.com/lindawangg/COVID-Net/blob/master/create_COVIDx.ipynb\n",
    "        df = df[~df['FILE NAME'].isin(discard)]\n",
    "        df = df[~df.URL.isin(_src0_url)]  # drop duplicates\n",
    "    del df['URL']\n",
    "    df.dropna(axis=0, how='any', inplace=True)\n",
    "    df['img'] = df.apply(src3_img, axis=1)\n",
    "    del df['FORMAT']\n",
    "    df.rename(columns={'FILE NAME': 'patient'}, inplace=True)\n",
    "    df['label'] = label\n",
    "    df['src'] = 3\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[50] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_etl_META_0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\n",
      "PythonRDD[151] at RDD at PythonRDD.scala:53\n",
      "[<src.rdd.CXR object at 0x0000022B97D527F0>]\n"
     ]
    }
   ],
   "source": [
    "from src.rdd import CXR\n",
    "from src.etl import *\n",
    "\n",
    "label = 'normal'\n",
    "print(label)\n",
    "rdd = spark.read.csv(INPUT_PATH_3_1_META, header=True, sep=\",\").rdd\n",
    "if label == 'covid':\n",
    "    # https://github.com/lindawangg/COVID-Net/blob/master/create_COVIDx.ipynb\n",
    "    rdd = rdd.filter(lambda x: x['FILE NAME'] not in discard)\n",
    "    rdd = rdd.filter(lambda x: x.URL not in _src0_url)\n",
    "rdd = rdd.map(lambda x: CXR(pid=x['FILE NAME'], fn_src=f\"{x['FILE NAME']}.{x.FORMAT.lower()}\", label=label, src=3))\n",
    "rdd = rdd.filter(lambda x: 1- x.isna())\n",
    "\n",
    "print(rdd)\n",
    "print(rdd.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NORMAL-2', 'NORMAL-2.png', 'normal', 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[1].pid, c[1].fn_src, c[1].label, c[1].src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-NTGREJQ9.attlocal.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x25cc3e64e48>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregate\n",
      "aggregateByKey\n",
      "barrier\n",
      "cache\n",
      "cartesian\n",
      "checkpoint\n",
      "coalesce\n",
      "cogroup\n",
      "collect\n",
      "collectAsMap\n",
      "collectWithJobGroup\n",
      "combineByKey\n",
      "context\n",
      "count\n",
      "countApprox\n",
      "countApproxDistinct\n",
      "countByKey\n",
      "countByValue\n",
      "ctx\n",
      "distinct\n",
      "filter\n",
      "first\n",
      "flatMap\n",
      "flatMapValues\n",
      "fold\n",
      "foldByKey\n",
      "foreach\n",
      "foreachPartition\n",
      "fullOuterJoin\n",
      "func\n",
      "getCheckpointFile\n",
      "getNumPartitions\n",
      "getStorageLevel\n",
      "glom\n",
      "groupBy\n",
      "groupByKey\n",
      "groupWith\n",
      "histogram\n",
      "id\n",
      "intersection\n",
      "isCheckpointed\n",
      "isEmpty\n",
      "isLocallyCheckpointed\n",
      "is_barrier\n",
      "is_cached\n",
      "is_checkpointed\n",
      "join\n",
      "keyBy\n",
      "keys\n",
      "leftOuterJoin\n",
      "localCheckpoint\n",
      "lookup\n",
      "map\n",
      "mapPartitions\n",
      "mapPartitionsWithIndex\n",
      "mapPartitionsWithSplit\n",
      "mapValues\n",
      "max\n",
      "mean\n",
      "meanApprox\n",
      "min\n",
      "name\n",
      "partitionBy\n",
      "partitioner\n",
      "persist\n",
      "pipe\n",
      "preservesPartitioning\n",
      "prev\n",
      "randomSplit\n",
      "reduce\n",
      "reduceByKey\n",
      "reduceByKeyLocally\n",
      "repartition\n",
      "repartitionAndSortWithinPartitions\n",
      "rightOuterJoin\n",
      "sample\n",
      "sampleByKey\n",
      "sampleStdev\n",
      "sampleVariance\n",
      "saveAsHadoopDataset\n",
      "saveAsHadoopFile\n",
      "saveAsNewAPIHadoopDataset\n",
      "saveAsNewAPIHadoopFile\n",
      "saveAsPickleFile\n",
      "saveAsSequenceFile\n",
      "saveAsTextFile\n",
      "setName\n",
      "sortBy\n",
      "sortByKey\n",
      "stats\n",
      "stdev\n",
      "subtract\n",
      "subtractByKey\n",
      "sum\n",
      "sumApprox\n",
      "take\n",
      "takeOrdered\n",
      "takeSample\n",
      "toDF\n",
      "toDebugString\n",
      "toLocalIterator\n",
      "top\n",
      "treeAggregate\n",
      "treeReduce\n",
      "union\n",
      "unpersist\n",
      "values\n",
      "variance\n",
      "zip\n",
      "zipWithIndex\n",
      "zipWithUniqueId\n"
     ]
    }
   ],
   "source": [
    "for i in dir(df.map(lambda x: x.url).distinct()):\n",
    "    if i[0] != '_':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkContext' object has no attribute 'sqlContext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-cd3d28729064>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msqlContext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqlContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkContext' object has no attribute 'sqlContext'"
     ]
    }
   ],
   "source": [
    "sqlContext = sc.sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.etl import *\n",
    "from src.utils import *\n",
    "import yaml\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['data']['image_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "META = etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('label distribution: ', Counter(META.label), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "META = dataset_split(META)\n",
    "META.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('label integer map: ', params['train']['labelmap'], sep='\\n', end='\\n\\n')\n",
    "for ds in (0, 1):\n",
    "    print('{0} data from covid datasets: '.format({0: 'test', 1: 'train'}[ds]), Counter(META[META.train==ds].label), sep='\\n', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache image meta\n",
    "if not os.path.isdir(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "with open(os.path.join(SAVE_PATH, 'meta'), 'wb') as pickle_file:\n",
    "    pickle.dump(META, pickle_file, pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"META data saved at {os.path.join(SAVE_PATH, 'meta')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if False:\n",
    "    # -------------------------------------------------------------------------------------------------------------------\n",
    "    # imagge file to matrix  \n",
    "    # NOTE: this needs model to load all data at once into cache which take up too much storage, \n",
    "    #       making computing inefficient, NOT GOOD\n",
    "    # -------------------------------------------------------------------------------------------------------------------\n",
    "    # train_data / test_data structure: \n",
    "    #                        {'covid': {'data': list of size*size*3 numpy array, 'label': corresponding list of integer},\n",
    "    #                        '!covid': {'data': list of size*size*3 numpy array, 'label': corresponding list of integer}}\n",
    "    TRAIN_DATA, TEST_DATA = datafromfile(META)\n",
    "    # a sanity check\n",
    "    print(f\"train covid sample #: {len(TRAIN_DATA['covid']['label'])}\")\n",
    "    print(f\"train !covid sample #: {len(TRAIN_DATA['!covid']['label'])}\")\n",
    "    print(f\"test  covid sample #: {len(TEST_DATA['covid']['label'])}\")\n",
    "    print(f\"test  !covid sample #: {len(TEST_DATA['!covid']['label'])}\")\n",
    "    # cache image data\n",
    "    with open(os.path.join(SAVE_PATH, 'train.data'), 'wb') as pickle_file:\n",
    "        pickle.dump(TRAIN_DATA, pickle_file, pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"Image data saved at {os.path.join(SAVE_PATH, 'train.data')}\")\n",
    "    with open(os.path.join(SAVE_PATH, 'test.data'), 'wb') as pickle_file:\n",
    "        pickle.dump(TEST_DATA, pickle_file, pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"Image data saved at {os.path.join(SAVE_PATH, 'test.data')}\")\n",
    "else:\n",
    "    # merge imgaes together instead, and transform image to batches of matrix during training\n",
    "    mergesoure(META)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for image shape, they need to consistent with model input\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "for i in np.random.choice(META.index, 10):\n",
    "    print(i, cv2.imread(META.loc[i, 'imgid']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
