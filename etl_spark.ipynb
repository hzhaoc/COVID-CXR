{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"COVID-CXR META ETL\") \\\n",
    "    .config(\"spark.some.config.option\", \"xxx\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    # once intialized, it's global in the jupyter notebook kernal; one context per notebook\n",
    "    sc = pyspark.SparkContext.getOrCreate('local[*]')\n",
    "except:\n",
    "    sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "sqlc = pyspark.SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.etl_spark import *\n",
    "\n",
    "rdd = spark_etl()\n",
    "a = rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22049"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CXRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pandas' from 'C:\\\\Users\\\\conno\\\\anaconda3\\\\envs\\\\covid\\\\lib\\\\site-packages\\\\pandas\\\\__init__.py'>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "META = pd.DataFrame()\n",
    "for i, cxr in enumerate(CXRs):\n",
    "    META.loc[i, 'patient'] = cxr.pid\n",
    "    META.loc[i, 'img'] = cxr.fn_src\n",
    "    META.loc[i, 'label'] = cxr.label\n",
    "    META.loc[i, 'src'] = cxr.src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-NTGREJQ9.attlocal.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>COVID-CXR META ETL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=COVID-CXR META ETL>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-NTGREJQ9.attlocal.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x25cc3e64e48>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregate\n",
      "aggregateByKey\n",
      "barrier\n",
      "cache\n",
      "cartesian\n",
      "checkpoint\n",
      "coalesce\n",
      "cogroup\n",
      "collect\n",
      "collectAsMap\n",
      "collectWithJobGroup\n",
      "combineByKey\n",
      "context\n",
      "count\n",
      "countApprox\n",
      "countApproxDistinct\n",
      "countByKey\n",
      "countByValue\n",
      "ctx\n",
      "distinct\n",
      "filter\n",
      "first\n",
      "flatMap\n",
      "flatMapValues\n",
      "fold\n",
      "foldByKey\n",
      "foreach\n",
      "foreachPartition\n",
      "fullOuterJoin\n",
      "getCheckpointFile\n",
      "getNumPartitions\n",
      "getStorageLevel\n",
      "glom\n",
      "groupBy\n",
      "groupByKey\n",
      "groupWith\n",
      "histogram\n",
      "id\n",
      "intersection\n",
      "isCheckpointed\n",
      "isEmpty\n",
      "isLocallyCheckpointed\n",
      "is_cached\n",
      "is_checkpointed\n",
      "join\n",
      "keyBy\n",
      "keys\n",
      "leftOuterJoin\n",
      "localCheckpoint\n",
      "lookup\n",
      "map\n",
      "mapPartitions\n",
      "mapPartitionsWithIndex\n",
      "mapPartitionsWithSplit\n",
      "mapValues\n",
      "max\n",
      "mean\n",
      "meanApprox\n",
      "min\n",
      "name\n",
      "partitionBy\n",
      "partitioner\n",
      "persist\n",
      "pipe\n",
      "randomSplit\n",
      "reduce\n",
      "reduceByKey\n",
      "reduceByKeyLocally\n",
      "repartition\n",
      "repartitionAndSortWithinPartitions\n",
      "rightOuterJoin\n",
      "sample\n",
      "sampleByKey\n",
      "sampleStdev\n",
      "sampleVariance\n",
      "saveAsHadoopDataset\n",
      "saveAsHadoopFile\n",
      "saveAsNewAPIHadoopDataset\n",
      "saveAsNewAPIHadoopFile\n",
      "saveAsPickleFile\n",
      "saveAsSequenceFile\n",
      "saveAsTextFile\n",
      "setName\n",
      "sortBy\n",
      "sortByKey\n",
      "stats\n",
      "stdev\n",
      "subtract\n",
      "subtractByKey\n",
      "sum\n",
      "sumApprox\n",
      "take\n",
      "takeOrdered\n",
      "takeSample\n",
      "toDF\n",
      "toDebugString\n",
      "toLocalIterator\n",
      "top\n",
      "treeAggregate\n",
      "treeReduce\n",
      "union\n",
      "unpersist\n",
      "values\n",
      "variance\n",
      "zip\n",
      "zipWithIndex\n",
      "zipWithUniqueId\n"
     ]
    }
   ],
   "source": [
    "for i in dir(rdd):\n",
    "    if i[0] != '_':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkContext' object has no attribute 'sqlContext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-cd3d28729064>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msqlContext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqlContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkContext' object has no attribute 'sqlContext'"
     ]
    }
   ],
   "source": [
    "sqlContext = sc.sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.etl import *\n",
    "from src.utils import *\n",
    "import yaml\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['data']['image_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "META = etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('label distribution: ', Counter(META.label), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "META = dataset_split(META)\n",
    "META.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('label integer map: ', params['train']['labelmap'], sep='\\n', end='\\n\\n')\n",
    "for ds in (0, 1):\n",
    "    print('{0} data from covid datasets: '.format({0: 'test', 1: 'train'}[ds]), Counter(META[META.train==ds].label), sep='\\n', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache image meta\n",
    "if not os.path.isdir(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "with open(os.path.join(SAVE_PATH, 'meta'), 'wb') as pickle_file:\n",
    "    pickle.dump(META, pickle_file, pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"META data saved at {os.path.join(SAVE_PATH, 'meta')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if False:\n",
    "    # -------------------------------------------------------------------------------------------------------------------\n",
    "    # imagge file to matrix  \n",
    "    # NOTE: this needs model to load all data at once into cache which take up too much storage, \n",
    "    #       making computing inefficient, NOT GOOD\n",
    "    # -------------------------------------------------------------------------------------------------------------------\n",
    "    # train_data / test_data structure: \n",
    "    #                        {'covid': {'data': list of size*size*3 numpy array, 'label': corresponding list of integer},\n",
    "    #                        '!covid': {'data': list of size*size*3 numpy array, 'label': corresponding list of integer}}\n",
    "    TRAIN_DATA, TEST_DATA = datafromfile(META)\n",
    "    # a sanity check\n",
    "    print(f\"train covid sample #: {len(TRAIN_DATA['covid']['label'])}\")\n",
    "    print(f\"train !covid sample #: {len(TRAIN_DATA['!covid']['label'])}\")\n",
    "    print(f\"test  covid sample #: {len(TEST_DATA['covid']['label'])}\")\n",
    "    print(f\"test  !covid sample #: {len(TEST_DATA['!covid']['label'])}\")\n",
    "    # cache image data\n",
    "    with open(os.path.join(SAVE_PATH, 'train.data'), 'wb') as pickle_file:\n",
    "        pickle.dump(TRAIN_DATA, pickle_file, pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"Image data saved at {os.path.join(SAVE_PATH, 'train.data')}\")\n",
    "    with open(os.path.join(SAVE_PATH, 'test.data'), 'wb') as pickle_file:\n",
    "        pickle.dump(TEST_DATA, pickle_file, pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"Image data saved at {os.path.join(SAVE_PATH, 'test.data')}\")\n",
    "else:\n",
    "    # merge imgaes together instead, and transform image to batches of matrix during training\n",
    "    mergesoure(META)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for image shape, they need to consistent with model input\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "for i in np.random.choice(META.index, 10):\n",
    "    print(i, cv2.imread(META.loc[i, 'imgid']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
